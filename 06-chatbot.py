"""
Nos Chatbots sempre enviamos o hist√≥rico da conversa anterior, onde o LLM utiliza como base 
para gerar o output(resposta). O hist√≥rico n√£o √© retido na OPENAi mas sim numa vari√°vel do 
c√≥digo em execu√ß√£o
Nos trechos onde temos o Append √© onde as resposta e perguntas do LLM s√£o armazenadas. O formato do c√≥digo √© .py
"""
import openai
from dotenv import load_dotenv
from colorama import Fore, Style, init

load_dotenv()

client = openai.Client()

# Inicializando o colorama

init(autoreset=True)

## 2) a fun√ß√£o abaixo chama o chat gpt para obter a resposta
def geracao_texto(mensagens):
    resposta = client.chat.completions.create(
        messages=mensagens,
        model="gpt-4.1-mini",
        temperature=1,
        max_tokens=1000,
        stream=True
    )
    print(f"{Fore.CYAN}Bot:", end="")
    texto_completo = ""
    for resposta_stream in resposta:
        texto = resposta_stream.choices[0].delta.content
        if texto:
            ## 3) A resposta da API vem em stream. O for resposta_stream in resposta: itera sobre cada peda√ßo (chunk) 
            # dessa nova resposta, e a linha print(texto, end="") exibe cada peda√ßo em tempo real na tela. 
            # O end="" faz com que cada pedacinho seja impresso na mesma linha, sem pular para a pr√≥xima, 
            # criando o efeito de digita√ß√£o.
            #A constru√ß√£o do texto_completo: Enquanto cada pedacinho da resposta √© exibido (print(texto)), 
            # a linha texto_completo += texto vai concatenando esses peda√ßos que servira para enviar todo o historico
            # ao chat gpt. 
            print(texto, end="")
            texto_completo += texto
    print()
    ## 4) ao termino da exibi√ß√£o toda a mensagem √© salva e agregada na variavel mensagens
    ## e retorna para o la√ßo while la embaixo
    ## Quando √© feita uma nova pergunta no la√ßo while la embaixo, √© enviado todo o historico
    ##para o modelo LLM atrav√©s da variavel mensagens.A l√≥gica de passar e atualizar a vari√°vel mensagens 
    # a cada itera√ß√£o √© a maneira correta de manter o hist√≥rico da conversa. 
    # Isso permite que o modelo de IA entenda o contexto das perguntas anteriores. 
    # LLM intepreta todo o historico e retorna tamb√©m a resposta com a ultima pergunta. 

    mensagens.append({"role":"assistant", "content": texto_completo})
    return mensagens

## 1) Essa √© a fun√ß√£o incial inicializada.
## Ao digitarmos a pergunta, a mesma √© adicionada a variavel mensagens, e
## chamada a fun√ß√£o geracao texto.

if __name__ == "__main__":
    print(f"{Fore.BLUE}Bem Vindo ao Chatbotü§ñ")
    mensagens = []
    while True:
        in_user = input(f"{Fore.GREEN}User: {Style.RESET_ALL} ")
        mensagens.append({"role": "user", "content": in_user})
        mensagens = geracao_texto(mensagens)
        print (mensagens)
